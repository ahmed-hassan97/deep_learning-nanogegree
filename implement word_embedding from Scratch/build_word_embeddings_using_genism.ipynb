{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "build_word_embeddings_using_genism.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOpstKjni3Ry"
      },
      "source": [
        "!wget https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDEiLULGi96a"
      },
      "source": [
        "!unzip /content/text8.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR3DD1vZjBe4"
      },
      "source": [
        "with open('/content/text8') as f:\n",
        "    text =  f.read()\n",
        "\n",
        "print(text[:100])    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHBi9Ka-jBnZ"
      },
      "source": [
        "# nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from  nltk.stem import SnowballStemmer\n",
        "\n",
        "# Word2vec\n",
        "import gensim\n",
        "\n",
        "# Utility\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "from collections import Counter\n",
        "import logging\n",
        "import time\n",
        "import pickle\n",
        "import itertools\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky6XhBJ9jBxH"
      },
      "source": [
        "nltk.download('stopwords')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP34tCE0jv0S"
      },
      "source": [
        "# WORD2VEC \n",
        "W2V_SIZE = 300\n",
        "W2V_WINDOW = 10\n",
        "W2V_EPOCH = 128\n",
        "W2V_MIN_COUNT = 10\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "\n",
        "# EXPORT\n",
        "WORD2VEC_MODEL = \"model.w2v\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fmc9yeVcj2Sp"
      },
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwat7jU5j-pt"
      },
      "source": [
        "def preprocess(text, stem=False):\n",
        "    # Remove link,user and special characters\n",
        "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token not in stop_words:\n",
        "            if stem:\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "text = preprocess(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWZEx9AnkjlS"
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbqeILZwuxz-"
      },
      "source": [
        "words = text.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7WI8WJ1kemD"
      },
      "source": [
        "documents = [_text.split() for _text in text.split()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wFhMSy7j-y4"
      },
      "source": [
        "documents[:30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP8bXPe0uVWg"
      },
      "source": [
        "word_counts = Counter(words)\n",
        "# sorting the words from most to least frequent in text occurrence\n",
        "sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "# create int_to_vocab dictionaries\n",
        "int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
        "vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_y8HYbru5wG"
      },
      "source": [
        "int_to_vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qns_95TRj-9R"
      },
      "source": [
        "w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, \n",
        "                                            window=W2V_WINDOW, \n",
        "                                            min_count=W2V_MIN_COUNT, \n",
        "                                            workers=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XEFI6GXk2TW"
      },
      "source": [
        "w2v_model.build_vocab(documents)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdma1tc5k2ZS"
      },
      "source": [
        "\n",
        "vocab_size = len(words)\n",
        "print(\"Vocab size\", vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEgqm9krlTrC"
      },
      "source": [
        "w2v_model.train(documents, total_examples=len(documents), epochs=50)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUbysz55_LSi"
      },
      "source": [
        "w2v_model.most_similar(\"seven\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZHGrkbdnobD"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0vfaUpgpEch"
      },
      "source": [
        "w2v_model.save('w2v_model.wv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJmPL5XZslzJ"
      },
      "source": [
        "X = w2v_model[w2v_model.wv.vocab]\n",
        "tsne = TSNE(n_components=2)\n",
        "embed_tsne = tsne.fit_transform(X[:1000,:])\n",
        "fig, ax = plt.subplots(figsize=(16, 16))\n",
        "for idx in range(viz_words):\n",
        "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
        "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}